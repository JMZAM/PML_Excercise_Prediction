<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>PML Excercise prediction by JMZAM</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">PML Excercise prediction</h1>
        <p class="header"></p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/JMZAM/PML_Excercise_Prediction/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/JMZAM/PML_Excercise_Prediction/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/JMZAM/PML_Excercise_Prediction">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by <a class="header name" href="https://github.com/JMZAM">JMZAM</a></p>


      </header>
      <section>
        <h1>
<a id="prediction-assignment" class="anchor" href="#prediction-assignment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prediction Assignment</h1>

<p>Juan M Zambrano<br>
October 2015  </p>

<h1>
<a id="project-overview" class="anchor" href="#project-overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project overview</h1>

<p>According to the projectÂ´s instructions:</p>

<blockquote>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the <a href="http://groupware.les.inf.puc-rio.br/har">website</a> (see the section on the Weight Lifting Exercise Dataset).</p>
</blockquote>

<p>The <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">training</a> and <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">test</a> sets were provided.</p>

<h1>
<a id="goal" class="anchor" href="#goal" aria-hidden="true"><span class="octicon octicon-link"></span></a>Goal</h1>

<p>To use data from accelerometers on the belt, forearm, arm and dumbell of 6 participants asked to perform barbell lifts in 5 different ways to predict the way in which they did the excercise.</p>

<h1>
<a id="initial-data-loading-and-creation-of-test-and-validation-sets" class="anchor" href="#initial-data-loading-and-creation-of-test-and-validation-sets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Initial data loading and creation of test and validation sets</h1>

<div class="highlight highlight-source-r"><pre><span class="pl-c">#First the packages that will be used are loaded</span>
library(<span class="pl-smi">caret</span>)
library(<span class="pl-smi">randomForest</span>)
library(<span class="pl-smi">class</span>)
library(<span class="pl-smi">nnet</span>)

<span class="pl-c">#The training and test data sets are loaded</span>
<span class="pl-smi">pml.training</span> <span class="pl-k">&lt;-</span> read.csv(<span class="pl-s"><span class="pl-pds">"</span>./pml-training.csv<span class="pl-pds">"</span></span>, <span class="pl-v">row.names</span><span class="pl-k">=</span><span class="pl-c1">1</span>, <span class="pl-v">stringsAsFactors</span> <span class="pl-k">=</span> <span class="pl-c1">FALSE</span>)
<span class="pl-smi">pml.testing</span> <span class="pl-k">&lt;-</span> read.csv(<span class="pl-s"><span class="pl-pds">"</span>./pml-testing.csv<span class="pl-pds">"</span></span>, <span class="pl-v">row.names</span><span class="pl-k">=</span><span class="pl-c1">1</span>, <span class="pl-v">stringsAsFactors</span> <span class="pl-k">=</span> <span class="pl-c1">FALSE</span>)

<span class="pl-c">#The supplied training set is subdivided into training and validation sets</span>
set.seed(<span class="pl-c1">1991991</span>)
<span class="pl-smi">traindata</span> <span class="pl-k">&lt;-</span> createDataPartition(<span class="pl-smi">pml.training</span><span class="pl-k">$</span><span class="pl-smi">classe</span>, <span class="pl-v">p</span> <span class="pl-k">=</span> <span class="pl-c1">0.8</span>, <span class="pl-v">list</span> <span class="pl-k">=</span> <span class="pl-c1">FALSE</span>)
<span class="pl-smi">training</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">pml.training</span>[<span class="pl-smi">traindata</span>, ]
<span class="pl-smi">validation</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">pml.training</span>[<span class="pl-k">-</span><span class="pl-smi">traindata</span>, ]</pre></div>

<h1>
<a id="preprocessing" class="anchor" href="#preprocessing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprocessing</h1>

<p>For feature selection, several parameters were taken into account:</p>

<table>
<thead>
<tr>
<th>Variable rule</th>
<th>Number of variables excluded for this reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. Descriptive variables were excluded from the analysis (i.e. variables that described the participant name, event time and date, etc.)</td>
<td>6</td>
</tr>
<tr>
<td>2. Variables that had less than 5% of values different from zero or NA were excluded</td>
<td>106</td>
</tr>
<tr>
<td>3. Variables with near nonzero variance were excluded.</td>
<td>0</td>
</tr>
</tbody>
</table>

<div class="highlight highlight-source-r"><pre><span class="pl-c">#character descriptive variables </span>
<span class="pl-smi">charvars</span> <span class="pl-k">&lt;-</span> c(<span class="pl-s"><span class="pl-pds">"</span>user_name<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>raw_timestamp_part_1<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>raw_timestamp_part_2<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>cvtd_timestamp<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>new_window<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>num_window<span class="pl-pds">"</span></span>)

<span class="pl-c">#variables with excess NA or black values</span>
<span class="pl-smi">nablankcount</span> <span class="pl-k">&lt;-</span> sapply(<span class="pl-smi">training</span>, <span class="pl-k">function</span>(<span class="pl-smi">x</span>) sum((is.na(<span class="pl-smi">x</span>) <span class="pl-k">|</span> <span class="pl-smi">x</span> <span class="pl-k">==</span> <span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>))) <span class="pl-c">#counts the number of NA/blanks per column</span>
<span class="pl-smi">nablankcols</span> <span class="pl-k">&lt;-</span> names(<span class="pl-smi">nablankcount</span>[<span class="pl-smi">nablankcount</span> <span class="pl-k">&gt;</span> <span class="pl-c1">0.05</span><span class="pl-k">*</span>dim(<span class="pl-smi">training</span>)[<span class="pl-c1">1</span>]]) <span class="pl-c">#names for variables with more than 5% NA/blank values</span>
<span class="pl-smi">training2</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">training</span>[,<span class="pl-k">!</span>names(<span class="pl-smi">training</span>) <span class="pl-k">%in%</span> c(<span class="pl-smi">charvars</span>,<span class="pl-smi">nablankcols</span>)]

<span class="pl-c">#variables with near zero variance</span>
<span class="pl-smi">near0var</span><span class="pl-k">&lt;-</span> nearZeroVar(<span class="pl-smi">training2</span>)
<span class="pl-k">if</span> (length(<span class="pl-smi">near0var</span>)<span class="pl-k">&gt;</span> <span class="pl-c1">0</span>) <span class="pl-smi">training2</span><span class="pl-k">&lt;-</span> <span class="pl-smi">training2</span>[,<span class="pl-k">-</span><span class="pl-smi">near0var</span>]

<span class="pl-c">#The selected features are uptated to the validation set</span>
<span class="pl-smi">finalcols</span><span class="pl-k">&lt;-</span>names(<span class="pl-smi">training2</span>)
<span class="pl-smi">validation2</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">validation</span>[,names(<span class="pl-smi">training2</span>)]</pre></div>

<h1>
<a id="model-fitting" class="anchor" href="#model-fitting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model fitting</h1>

<p>After reviewing several of the machine learning techniques that were introduced in class, along with others observed while studying the options in the caret package, I decided to include 4 types of algorithms to compare and choose the one with the best accuracy in classification when evaluating the out-of-sample error with the validation set. That is, models with accuracy &gt; 95%. The train algorithm within the caret package uses by default the accuracy and kappa parameters for model selection (cross validation).</p>

<p>The first model attempted is a classification tree model:</p>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">modFit</span> <span class="pl-k">&lt;-</span> train(as.factor(<span class="pl-smi">classe</span>) <span class="pl-k">~</span> ., <span class="pl-v">method</span> <span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>rpart<span class="pl-pds">"</span></span>, <span class="pl-v">data</span><span class="pl-k">=</span><span class="pl-smi">training2</span>, <span class="pl-v">trControl</span> <span class="pl-k">=</span> trainControl(<span class="pl-v">method</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>cv<span class="pl-pds">"</span></span>))</pre></div>

<pre><code>## Loading required namespace: e1071
</code></pre>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">ptraining</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">modFit</span>, <span class="pl-smi">validation2</span>)
confusionMatrix(<span class="pl-smi">ptraining</span>, <span class="pl-smi">validation2</span><span class="pl-k">$</span><span class="pl-smi">classe</span>)</pre></div>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1019  335  323  292  110
##          B   17  228   23  112   90
##          C   80  196  338  239  195
##          D    0    0    0    0    0
##          E    0    0    0    0  326
## 
## Overall Statistics
##                                           
##                Accuracy : 0.4871          
##                  95% CI : (0.4714, 0.5029)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.3289          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9131  0.30040  0.49415   0.0000   0.4521
## Specificity            0.6224  0.92351  0.78080   1.0000   1.0000
## Pos Pred Value         0.4901  0.48511  0.32252      NaN   1.0000
## Neg Pred Value         0.9474  0.84622  0.87965   0.8361   0.8902
## Prevalence             0.2845  0.19347  0.17436   0.1639   0.1838
## Detection Rate         0.2598  0.05812  0.08616   0.0000   0.0831
## Detection Prevalence   0.5300  0.11981  0.26714   0.0000   0.0831
## Balanced Accuracy      0.7677  0.61195  0.63747   0.5000   0.7261
</code></pre>

<p>As we can see, the accuracy is very low (48.71%); for this reason the model will be discarded.
The second model is a single-hidden-layer neural network. We will use a maximum of 1000 iterations for time considerations.</p>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">modFit2</span> <span class="pl-k">&lt;-</span> nnet(as.factor(<span class="pl-smi">classe</span>)<span class="pl-k">~</span>., <span class="pl-v">data</span><span class="pl-k">=</span><span class="pl-smi">training2</span>, <span class="pl-v">size</span> <span class="pl-k">=</span> <span class="pl-c1">17</span>, <span class="pl-v">decay</span> <span class="pl-k">=</span> <span class="pl-c1">5e-4</span>, <span class="pl-v">maxit</span> <span class="pl-k">=</span> <span class="pl-c1">1000</span>)</pre></div>

<pre><code>## # weights:  991
## initial  value 33673.300036 
## iter  10 value 22691.761091
## iter  20 value 21847.599347
## iter  30 value 21265.107361
## iter  40 value 20654.274834
## iter  50 value 20262.791545
## iter  60 value 19919.662994
## iter  70 value 19661.567214
## iter  80 value 19408.599568
## iter  90 value 18993.272933
## iter 100 value 18733.308901
## iter 110 value 18252.452766
## iter 120 value 17850.261296
## iter 130 value 17611.439885
## iter 140 value 17388.097943
## iter 150 value 17244.000297
## iter 160 value 17104.982919
## iter 170 value 16909.056823
## iter 180 value 16776.100006
## iter 190 value 16549.962531
## iter 200 value 16369.305983
## iter 210 value 16233.548337
## iter 220 value 16115.475770
## iter 230 value 16040.870853
## iter 240 value 15918.249529
## iter 250 value 15777.678053
## iter 260 value 15644.226408
## iter 270 value 15547.668271
## iter 280 value 15388.684110
## iter 290 value 15241.464646
## iter 300 value 15162.367083
## iter 310 value 15051.969632
## iter 320 value 14938.183849
## iter 330 value 14787.009926
## iter 340 value 14639.919577
## iter 350 value 14553.243056
## iter 360 value 14430.040223
## iter 370 value 14354.801717
## iter 380 value 14247.105311
## iter 390 value 14192.135633
## iter 400 value 14113.643788
## iter 410 value 14041.545568
## iter 420 value 13962.609623
## iter 430 value 13892.075609
## iter 440 value 13834.211625
## iter 450 value 13781.625770
## iter 460 value 13724.211062
## iter 470 value 13688.152173
## iter 480 value 13654.596613
## iter 490 value 13626.667697
## iter 500 value 13592.951207
## iter 510 value 13566.907382
## iter 520 value 13543.725073
## iter 530 value 13530.124974
## iter 540 value 13516.530930
## iter 550 value 13500.933312
## iter 560 value 13496.249340
## iter 570 value 13487.793780
## iter 580 value 13483.223783
## iter 590 value 13480.294457
## iter 600 value 13476.582962
## iter 610 value 13474.368201
## iter 620 value 13471.650329
## iter 630 value 13471.238091
## iter 640 value 13470.317147
## iter 650 value 13463.211708
## iter 660 value 13461.554825
## iter 670 value 13460.461198
## iter 680 value 13459.706755
## iter 690 value 13459.345756
## iter 700 value 13458.767287
## iter 710 value 13458.436818
## iter 720 value 13457.862327
## iter 730 value 13457.362059
## iter 740 value 13456.182867
## iter 750 value 13452.927726
## iter 760 value 13451.329744
## iter 770 value 13448.955501
## iter 780 value 13448.521672
## iter 790 value 13447.435577
## iter 800 value 13446.113477
## iter 810 value 13445.690862
## iter 820 value 13445.171429
## iter 830 value 13444.954042
## iter 840 value 13444.583287
## iter 850 value 13444.226226
## iter 860 value 13443.834208
## iter 870 value 13443.714975
## iter 880 value 13443.599699
## iter 890 value 13442.826183
## iter 900 value 13442.730310
## iter 910 value 13442.693601
## iter 920 value 13441.626628
## iter 930 value 13440.579911
## iter 940 value 13437.503938
## iter 950 value 13436.339121
## iter 960 value 13435.605067
## iter 970 value 13434.356017
## iter 980 value 13433.974406
## iter 990 value 13433.741257
## iter1000 value 13433.317277
## final  value 13433.317277 
## stopped after 1000 iterations
</code></pre>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">ptraining2</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">modFit2</span>, <span class="pl-smi">validation2</span>, <span class="pl-v">type</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>class<span class="pl-pds">"</span></span>)
confusionMatrix(<span class="pl-smi">ptraining2</span>, <span class="pl-smi">validation2</span><span class="pl-k">$</span><span class="pl-smi">classe</span>)</pre></div>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 945 157  53  75  32
##          B  42 390  52  37 110
##          C  47  67 422  92  93
##          D  49  30 100 405  68
##          E  33 115  57  34 418
## 
## Overall Statistics
##                                           
##                Accuracy : 0.6577          
##                  95% CI : (0.6426, 0.6725)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5652          
##  Mcnemar's Test P-Value : 4.681e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.8468  0.51383   0.6170   0.6299   0.5798
## Specificity            0.8871  0.92383   0.9077   0.9247   0.9254
## Pos Pred Value         0.7488  0.61807   0.5853   0.6212   0.6362
## Neg Pred Value         0.9357  0.88791   0.9182   0.9272   0.9072
## Prevalence             0.2845  0.19347   0.1744   0.1639   0.1838
## Detection Rate         0.2409  0.09941   0.1076   0.1032   0.1066
## Detection Prevalence   0.3217  0.16085   0.1838   0.1662   0.1675
## Balanced Accuracy      0.8669  0.71883   0.7623   0.7773   0.7526
</code></pre>

<p>As we can see, the accuracy has increased to 65.77%, however it is still unsatisfactory.
The third model is a k-nearest neighbour classification, which for each row of the test set finds the k nearest training vectors and decides classification upon majority vote. Given the amount of data, I will use k=1. </p>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">ptraining3</span> <span class="pl-k">&lt;-</span> knn(<span class="pl-smi">training2</span>[,<span class="pl-k">-</span><span class="pl-c1">53</span>], <span class="pl-smi">validation2</span>[,<span class="pl-k">-</span><span class="pl-c1">53</span>], <span class="pl-smi">training2</span>[,<span class="pl-c1">53</span>], <span class="pl-v">k</span> <span class="pl-k">=</span> <span class="pl-c1">1</span>, <span class="pl-v">prob</span> <span class="pl-k">=</span> <span class="pl-c1">FALSE</span>, <span class="pl-v">use.all</span> <span class="pl-k">=</span> <span class="pl-c1">TRUE</span>)
confusionMatrix(<span class="pl-smi">ptraining3</span>, <span class="pl-smi">validation2</span><span class="pl-k">$</span><span class="pl-smi">classe</span>)</pre></div>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1103   10    3    5    3
##          B    3  731   11    2    8
##          C    3   12  660   19    6
##          D    6    2    6  612    6
##          E    1    4    4    5  698
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9697          
##                  95% CI : (0.9638, 0.9748)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9616          
##  Mcnemar's Test P-Value : 0.1977          
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9884   0.9631   0.9649   0.9518   0.9681
## Specificity            0.9925   0.9924   0.9877   0.9939   0.9956
## Pos Pred Value         0.9813   0.9682   0.9429   0.9684   0.9803
## Neg Pred Value         0.9954   0.9912   0.9926   0.9906   0.9928
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2812   0.1863   0.1682   0.1560   0.1779
## Detection Prevalence   0.2865   0.1925   0.1784   0.1611   0.1815
## Balanced Accuracy      0.9904   0.9778   0.9763   0.9728   0.9819
</code></pre>

<p>The out-of-sample error has decreased significantly! This model gives us an accuracy of over 95%, to be exact 96.97%.
Though we have achieved an acceptable performance, it is worth to try another algorithm, the random forest:</p>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">modFit4</span> <span class="pl-k">&lt;-</span> randomForest(as.factor(<span class="pl-smi">classe</span>) <span class="pl-k">~</span> ., <span class="pl-v">data</span> <span class="pl-k">=</span> <span class="pl-smi">training2</span>, <span class="pl-v">importance</span> <span class="pl-k">=</span> <span class="pl-c1">TRUE</span>, <span class="pl-v">ntrees</span> <span class="pl-k">=</span> <span class="pl-c1">10</span>, <span class="pl-v">trControl</span> <span class="pl-k">=</span> trainControl(<span class="pl-v">method</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>cv<span class="pl-pds">"</span></span>))
<span class="pl-smi">ptraining4</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">modFit4</span>, <span class="pl-smi">validation2</span>)
confusionMatrix(<span class="pl-smi">ptraining4</span>, <span class="pl-smi">validation2</span><span class="pl-k">$</span><span class="pl-smi">classe</span>)</pre></div>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1116    3    0    0    0
##          B    0  754    1    0    0
##          C    0    2  682   10    0
##          D    0    0    1  631    3
##          E    0    0    0    2  718
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9944          
##                  95% CI : (0.9915, 0.9965)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9929          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9934   0.9971   0.9813   0.9958
## Specificity            0.9989   0.9997   0.9963   0.9988   0.9994
## Pos Pred Value         0.9973   0.9987   0.9827   0.9937   0.9972
## Neg Pred Value         1.0000   0.9984   0.9994   0.9964   0.9991
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2845   0.1922   0.1738   0.1608   0.1830
## Detection Prevalence   0.2852   0.1925   0.1769   0.1619   0.1835
## Balanced Accuracy      0.9995   0.9965   0.9967   0.9901   0.9976
</code></pre>

<p>As we can see, this is the model with the highest accuracy, to be precise 99.44%. This is the model that I will choose for the predictions.</p>

<h1>
<a id="the-final-predictions" class="anchor" href="#the-final-predictions" aria-hidden="true"><span class="octicon octicon-link"></span></a>The final predictions</h1>

<p>In conclusion, there seem to be two models with an adequate level of accuracy (k-nearest neighbour and random forest algorithms). We can use these two to predict the test data and identify possible differences.</p>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">test</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">pml.testing</span>[,names(<span class="pl-smi">pml.testing</span>) <span class="pl-k">%in%</span> <span class="pl-smi">finalcols</span>]
<span class="pl-smi">knnpred</span> <span class="pl-k">&lt;-</span> knn(<span class="pl-smi">training2</span>[,<span class="pl-k">-</span><span class="pl-c1">53</span>], <span class="pl-smi">test</span>, <span class="pl-smi">training2</span>[,<span class="pl-c1">53</span>], <span class="pl-v">k</span> <span class="pl-k">=</span> <span class="pl-c1">1</span>, <span class="pl-v">prob</span> <span class="pl-k">=</span> <span class="pl-c1">FALSE</span>, <span class="pl-v">use.all</span> <span class="pl-k">=</span> <span class="pl-c1">TRUE</span>)
<span class="pl-smi">rfpred</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">modFit4</span>, <span class="pl-smi">test</span>)
setdiff(<span class="pl-smi">knnpred</span>,<span class="pl-smi">rfpred</span>)</pre></div>

<pre><code>## character(0)
</code></pre>

<p>There are no differences!
For our purposes, we had decided on using the random forest model. The predictions are:</p>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">rfpred</span></pre></div>

<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
</code></pre>

<p>with a model with an out of sample error of ~0.56%.</p>
      </section>
      <footer>
        <p><small>Hosted on <a href="https://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		
  </body>
</html>
